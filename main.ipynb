{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os.path\n",
    "\n",
    "def download_sound(word, dirpath=\"public/sounds\", slow=False):\n",
    "    temp_file = f\"{dirpath}/{word}.mp3\"\n",
    "    try:\n",
    "        if not os.path.isfile(temp_file):\n",
    "            # Create a gTTS object with Chinese language\n",
    "            tts = gTTS(text=word, lang='zh-TW', slow=slow)\n",
    "            # Save the audio file temporarily\n",
    "            tts.save(temp_file)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def extract_stroke_order(character=\"天\", dirpath=\"public\"):\n",
    "    if len(character) > 1:\n",
    "        for c in character: extract_stroke_order(c)\n",
    "        return\n",
    "    elif len(character) == 0: return\n",
    "    \n",
    "    animation_path = f\"{dirpath}/animation/{character}.gif\"\n",
    "    stroke_path = f\"{dirpath}/stroke/{character}.png\"\n",
    "    \n",
    "    if os.path.isfile(animation_path) and os.path.isfile(stroke_path):\n",
    "        return []\n",
    "    \n",
    "    base_url = \"https://www.strokeorder.com\"\n",
    "    url = f\"{base_url}/chinese/{character}\"\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all divs with the specified class\n",
    "        target_divs = soup.find_all('div', class_='stroke-article-content')\n",
    "\n",
    "        # Extract image sources from these divs\n",
    "        image_sources = []\n",
    "        for div in target_divs:\n",
    "            images = div.find_all('img')\n",
    "            for img in images:\n",
    "                src = img.get('src')\n",
    "                if src:  # Only add if src exists\n",
    "                    image_sources.append(src)\n",
    "\n",
    "        if image_sources:\n",
    "            for src in image_sources:\n",
    "                if src.startswith(\"/assets/bishun/animation/\"):\n",
    "                    try:\n",
    "                        absolute_url = urljoin(base_url, src)\n",
    "                        img_response = requests.get(absolute_url, headers=headers)\n",
    "                        img_response.raise_for_status()\n",
    "                        \n",
    "                        with open(animation_path, \"wb\") as f:\n",
    "                            f.write(img_response.content)\n",
    "                    except Exception as e: print(str(e))\n",
    "                elif src.startswith(\"/assets/bishun/stroke/\"):\n",
    "                    try:\n",
    "                        absolute_url = urljoin(base_url, src)\n",
    "                        img_response = requests.get(absolute_url, headers=headers)\n",
    "                        img_response.raise_for_status()\n",
    "                        \n",
    "                        with open(stroke_path, \"wb\") as f:\n",
    "                            f.write(img_response.content)\n",
    "                    except: pass\n",
    "            \n",
    "        return image_sources\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qǐngwèn', 'qìngwèn', 'qíngwèn']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "import itertools\n",
    "\n",
    "def get_pinyin(word):\n",
    "    result = pinyin(word, heteronym=True)\n",
    "    combinations = itertools.product(*result)\n",
    "    result = list(combinations)\n",
    "    return [\"\".join(r) for r in result]\n",
    "    \n",
    "get_pinyin(\"請問\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: httpx>=0.27.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Permisi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googletrans\n",
    "\n",
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "async def translate(text, dest=\"id\"):\n",
    "    async with Translator() as translator:\n",
    "        result = await translator.translate(text, src=\"zh-tw\", dest=dest)\n",
    "        return result.text\n",
    "    \n",
    "res = await translate(\"請問\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "def permute(segment = \"[在] [安寧,特曼,我,他,你,我們,他們,你們] [家]\"):\n",
    "    parts = segment.split(\" \")\n",
    "    parts = [p.removeprefix(\"[\").removesuffix(\"]\").split(\";\") for p in parts]\n",
    "    combinations = itertools.product(*parts)\n",
    "    return list(combinations)\n",
    "\n",
    "BASE_URL = \"https://pratamov.github.io/zh-tw\"\n",
    "BASE_URL = \"http://localhost:3000/zh-tw\"\n",
    "\n",
    "result = {}\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip():\n",
    "            title, segment = line.strip().split(\" -> \")\n",
    "            if title not in result:\n",
    "                result[title] = []\n",
    "            result[title] += permute(segment=segment.strip())\n",
    "\n",
    "keywords = [\"?\", \"!\", \",\"]\n",
    "dictionary = {}\n",
    "decks = []\n",
    "\n",
    "if not os.path.exists(\"dictionary.json\"):\n",
    "    with open(\"dictionary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "with open(\"dictionary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dictionary = json.load(f)\n",
    "    \n",
    "with open(\"data.result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for k, v in result.items():\n",
    "        \n",
    "        words = set()\n",
    "        for item in v: words.update(item)\n",
    "        for item in keywords: words.discard(item)\n",
    "        \n",
    "        f.write(f\"# !{k}:{len(v)}-ITEMS:{len(words)}-WORDS:{words}\\n\")\n",
    "        \n",
    "        result_item = {\n",
    "            \"title\": k,\n",
    "            \"item_count\": len(v),\n",
    "            \"word_count\": len(words),\n",
    "            \"items\": []\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in keywords:\n",
    "                if word not in dictionary:\n",
    "                    chars = [c for c in word]\n",
    "                    animation = [f\"{BASE_URL}/animation/{c}.gif\" for c in chars]\n",
    "                    stroke = [f\"{BASE_URL}/stroke/{c}.png\" for c in chars]\n",
    "                    dictionary[word] = {\n",
    "                        \"animation\": animation,\n",
    "                        \"stroke\": stroke,\n",
    "                        \"meaning\": await translate(word),\n",
    "                        \"pinyin\": get_pinyin(word)\n",
    "                    }\n",
    "        \n",
    "        for item in v:\n",
    "            sentence = \"\".join([i for i in item if i not in keywords])\n",
    "            download_sound(sentence)\n",
    "            extract_stroke_order(sentence)\n",
    "            item_str = \" \".join([i for i in item])\n",
    "            f.write(f\"{item_str}\\t{sentence}\\n\")\n",
    "            result_item[\"items\"].append({\n",
    "                \"item\": item_str,\n",
    "                \"sound\": f\"{BASE_URL}/sounds/{sentence}.mp3\"\n",
    "            })\n",
    "        \n",
    "        decks.append(result_item)\n",
    "\n",
    "with open(\"decks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(decks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"dictionary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dictionary, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "js_content = f\"\"\"const contentData = {json.dumps(decks, indent=2, ensure_ascii=False)};\\nexport default contentData;\"\"\"\n",
    "with open(\"src/data/contents.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(js_content)\n",
    "\n",
    "js_content = f\"\"\"const dictionaryData = {json.dumps(dictionary, indent=2, ensure_ascii=False)};\\nexport default dictionaryData;\"\"\"\n",
    "with open(\"src/data/dictionary.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(js_content)\n",
    "\n",
    "# def import_decks(filepath=\"decks.txt\"):\n",
    "#     decks, _decks = {\"decks\": []}, {}\n",
    "#     with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f.read().split(\"\\n\"):\n",
    "#             if not line.startswith(\"#\"):\n",
    "#                 segments = line.split(\"\\t\")\n",
    "#                 if len(segments) == 5:\n",
    "#                     try:\n",
    "#                         download_sound(segments[0])\n",
    "#                         extract_stroke_order(segments[4].strip())\n",
    "                        \n",
    "#                         if not os.path.os.path.isfile(f\"public/sounds/{segments[0].strip()}.mp3\"):\n",
    "#                             continue\n",
    "                        \n",
    "#                         is_complete = True\n",
    "#                         for char in segments[4].strip():\n",
    "#                             if not os.path.os.path.isfile(f\"public/stroke/{char}.png\"):\n",
    "#                                 is_complete = False\n",
    "#                             if not os.path.os.path.isfile(f\"public/animation/{char}.gif\"):\n",
    "#                                 is_complete = False\n",
    "#                         if not is_complete: continue\n",
    "                        \n",
    "#                         chars = [c for c in segments[4].strip()]\n",
    "#                         strokes = [f\"stroke/{c}.png\" for c in chars]\n",
    "#                         animations = [f\"animation/{c}.gif\" for c in chars]\n",
    "                        \n",
    "#                         if segments[2].strip() not in _decks: \n",
    "#                             _decks[segments[2].strip()] = []\n",
    "                            \n",
    "#                         _decks[segments[2].strip()].append({\n",
    "#                             \"mp3\": f\"sounds/{segments[0].strip()}.mp3\",\n",
    "#                             \"word\": segments[0].strip(),\n",
    "#                             \"means\": segments[1].strip(),\n",
    "#                             \"pinyin\": segments[3].strip(),\n",
    "#                             \"stroke\": strokes,\n",
    "#                             \"animation\": animations\n",
    "#                         })\n",
    "#                     except: print(f\"Error {line}\")\n",
    "#                 else: print(line)\n",
    "#     for k, v in _decks.items():\n",
    "#         decks[\"decks\"].append({\n",
    "#             \"name\": k,\n",
    "#             \"items\": v\n",
    "#         })\n",
    "        \n",
    "        \n",
    "#     with open(\"public/decks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(decks, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "#     js_content = f\"\"\"const data = {json.dumps(decks, indent=2, ensure_ascii=False)};\\nexport default data;\"\"\"\n",
    "#     with open(\"src/data/data.js\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(js_content)\n",
    "    \n",
    "# import_decks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
